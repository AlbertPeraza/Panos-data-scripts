{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling and Extracting Data from Websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module uses a set of non-standard libraries, which need to be installed on your machine. By default, your instance should have these installed, but if this is not the case, type these in the Unix shell prompt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo -H apt-get -y install libxml2-dev libxslt-dev python3-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo -H pip3 install -U lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo -H pip3 install -U pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the headlines from ESPN.com\n",
    "\n",
    "Let's start by trying to fetch the headlines from the site ESPN.com.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # This command allows us to fetch URLs\n",
    "from lxml import html # This module will allow us to parse the returned HTML/XML\n",
    "import pandas # To create a dataframe\n",
    "\n",
    "# Let's start by fetching the page, and parsing it\n",
    "url = \"http://www.espn.com/\"\n",
    "response = requests.get(url) # get the html of that url\n",
    "doc = html.fromstring(response.text) # parse it and create a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `doc` variable is an `HtmlElement` object, and we can now use **XPath** queries to locate the elements that we need. (Depending on time, we may do in class a tutorial on XPath. For now, you can look at the [W3Schools tutorial](http://www.w3schools.com/xpath/xpath_nodes.asp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to find all the `<a ...> ... </a>` tags in the returned html, which store the links in the page, we issue the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = doc.xpath(\"//a\")\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnk = links[70]\n",
    "type(lnk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lnk` variable is again an HtmlElement. To get parts of the html element that we need, we can use the `get` method (e.g., to get the `href` attribute) and the `text_content()` method (to get the text within the `<a>...</a>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnk.get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnk.text_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's revisit the _list comprehension_ approach that we discussed in the Python Primer session, for quickly constructing lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [lnk.get(\"href\") for lnk in doc.xpath('//a')]\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Use a list compresension approach, to get the text_content of all the URLs in the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now create a list where we put together text content and the URL for each link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's get the headlines..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine how we can get the data from the website. The key is to understand the structure of the HTML, where the data that we need is stored, and how to fetch the elements. Then, using the appropriate XPath queries, we will get what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # This command allows us to fetch URLs\n",
    "from lxml import html # This module will allow us to parse the returned HTML/XML\n",
    "import pandas # To create a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by fetching the page, and parsing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.espn.com/\"\n",
    "response = requests.get(url) # get the html of that url\n",
    "doc = html.fromstring(response.text) # parse it and create a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the `\"Right-Click > Inspect\"` option of Chrome,\n",
    "we right click on the headlines and select `\"Inspect\"`.\n",
    "This opens the source code.\n",
    "There we see that all under a `<div class=\"headlineStack\">` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlineNode = doc.xpath('//div[@class=\"headlineStack\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of that operation is a list with 9 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(headlineNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(headlineNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each headline is under a  `<li><a href=\"....\"></a>` tag.\n",
    "So, we get all the `<li><a ...>` tags within the `<div class=\"headlineStack\">`\n",
    "(which is stored in the \"`headlineNode`\" variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = headlineNode[0].xpath('.//li/a')\n",
    "len(headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the nodes with the conent in the headlines variable.\n",
    "We extract the text and the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\"Title\": a.text_content(), \"URL\": a.get(\"href\")} for a in headlines]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's create our dataframe, so that we can have a better view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pandas.DataFrame(data)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Of course, there are always more than one way to skin a cat...\n",
    "\n",
    "Alternatively, if we did not want to restrict ourselves to just the first headline box, we could write an alternative query, to get back all the headlines, that appear in an XPath `//div[@class=\"headlineStack\"]//li/a`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = doc.xpath('//div[@class=\"headlineStack\"]//li/a')\n",
    "data = [{\"Title\": a.text_content(), \"URL\": \"http://www.espn.com\" + a.get(\"href\")} for a in headlines]\n",
    "df = pandas.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Class Example: Crawl BuzzFeed\n",
    "\n",
    "* We will try to get the top articles that appear on Buzzfeed\n",
    "* We will grab the link for the article, the text of the title, the description, and the editor.\n",
    "* The results will be stored in a dataframe (we will see in detail what a dataframe is, in a couple of modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "resp = requests.get(\"http://www.buzzfeed.com\")\n",
    "doc = html.fromstring(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Solution for Buzzfeed (as of February 27, 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # This command allows us to fetch URLs\n",
    "from lxml import html # This module will allow us to parse the returned HTML/XML\n",
    "import pandas\n",
    "\n",
    "# Let's start by fetching the page, and parsing it\n",
    "url = \"http://www.buzzfeed.com/\"\n",
    "response = requests.get(url) # get the html of that url\n",
    "doc = html.fromstring(response.text) # parse it and create a document\n",
    "\n",
    "articleNodes = doc.xpath(\"//div[@class='xs-px05 sm-pl05']\") \n",
    "\n",
    "def parseArticleNode(article):\n",
    "    headline = article.xpath(\".//a/h2\")[0].text_content()\n",
    "    headline_link = article.xpath(\".//a\")[0].get(\"href\")\n",
    "    description = article.xpath(\".//a/p\")[0].text_content()\n",
    "    editor = article.xpath(\".//a[contains(@class,'card__byline-link')]/span\")[0].text_content().strip()\n",
    "    \n",
    "    result = {\n",
    "        \"headline\": headline,\n",
    "        \"URL\" : headline_link,\n",
    "        \"description\" : description,\n",
    "        \"editor\" : editor\n",
    "    }\n",
    "    return result\n",
    "\n",
    "data = [parseArticleNode(article) for article in articleNodes]\n",
    "df = pandas.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
