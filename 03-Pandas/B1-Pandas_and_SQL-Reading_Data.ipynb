{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pandas together with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the SQLAlchemy library if it is not installed\n",
    "# !sudo -H pip3 install -U sqlalchemy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make the graphs a bit prettier, and bigger\n",
    "matplotlib.style.use(['seaborn-talk', 'seaborn-ticks', 'seaborn-whitegrid'])\n",
    "plt.rcParams['figure.figsize'] = (15, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Importing SQL results into DataFrames using read_sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "The `read_sql` function of Pandas allows us to create a dataframe directly from a SQL query. To execute the query, we first setup the connection to the database using the SQLAlchemy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conn_string_imdb = 'mysql://{user}:{password}@{host}:{port}/{db}?charset=utf8'.format(\n",
    "    user='student', \n",
    "    password='dwdstudent2015', \n",
    "    host = 'db.ipeirotis.org', \n",
    "    port=3306, \n",
    "    db='imdb',\n",
    "    encoding = 'utf-8'\n",
    ")\n",
    "engine_imdb = create_engine(conn_string_imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's start with a simple example. We issue an SQL query, and get back the results loaded in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT * FROM actors LIMIT 10\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_actors = pd.read_sql(query, con=engine_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(df_actors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Aggregation Calculations: Pandas or SQL? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's work on a slightly more advanced example. We want to analyze the number of movies over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Basic Option: Fetch all data, analyze in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's do the simple thing first. We will fetch all the data from the movies table and then do a pivot table on top. Since we care about efficiency, we will also time the operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "query = '''SELECT * FROM movies'''\n",
    "df_basic = pd.read_sql(query, con=engine_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(df_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So, notice that it takes 2-3 seconds to fetch the data from SQL and create the dataframe, as we need to fetch almost 400K records. \n",
    "\n",
    "Once we have the records, we can then compute a pivot table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Counting movie IDs returns all the movies within the year\n",
    "# Counting movie ranks returns all the movies that have \n",
    "# a non-empty \"rank\" value (i.e., they have been rated)\n",
    "pivot = df_basic.pivot_table(\n",
    "    index = 'year',\n",
    "    aggfunc = 'count',\n",
    "    values = ['id', 'rank']\n",
    ")\n",
    "# Rename the columns\n",
    "pivot.columns = ['all_movies', 'rated_movies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# And let's check a few lines of the table\n",
    "pivot.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And we can then plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pivot.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Better option: Aggregation in SQL, fetch only necessary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's push the computation on the SQL server instead, using a GROUP BY and COUNT aggregates in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "query = '''\n",
    "SELECT year, COUNT(*) AS all_movies, COUNT(rank) AS rated_movies\n",
    "FROM movies \n",
    "GROUP BY year\n",
    "ORDER BY year\n",
    "'''\n",
    "df_movies = pd.read_sql(query, con=engine_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(df_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_movies.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Notice that the same calculation was done in a few (4-5) **milliseconds**. The SQL query that we used earlier it took **seconds** to execute. In fact, the **pivot** table calculation, executed after fetching all the data took longer than executing the GROUPBY/COUNT SQL query and fetching the results.\n",
    "\n",
    "While in this example the difference is negligible, once you deal with datasets that have millions, or tens of millions of rows, the savings become material and significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Plotting: The importance of index\n",
    "\n",
    "Let's try to plot the results. In pandas, the simple `plot()` command will use the index as the x-axis, and will plot all the numeric columns, as a line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The plot() command takes the index (the first \"column\") of the dataframe\n",
    "# and makes that the x-axis.\n",
    "# Then it plots *ALL* the numeric columns as a line\n",
    "df_movies.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We do not want to plot the `year` variable as a line. So, we select just the other two columns and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# First step: We can eliminate the \"year\" line by selecting \n",
    "# the columns that we want to plot\n",
    "# To select columns, we pass a list of the column names that\n",
    "# we want to keep in square brackets\n",
    "df_movies[ [\"all_movies\", \"rated_movies\"] ].plot() \n",
    "# still the x-axis does not list the year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A bit better. `year` is not appearing anymore, but we still do not have `year` as the x-axis. \n",
    "\n",
    "To make `year` the x-axis, we need to make it the index of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_movies_2 = df_movies.set_index('year')\n",
    "df_movies_2.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now the plot has the year as the x-axis, and the labels are proper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_movies_2.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### (Optional, but useful) Changing data types: Int vs Datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In our index above, the \"year\" variable is an integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_movies_2.index.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is mostly fine, but we can leverage the time series processing capabilities of Pandas by converting `year` to a date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# We first convert the index into datetime.\n",
    "df_movies_2.index = pd.to_datetime(df_movies_2.index, format='%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_movies_2.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we can do the `resample` the dates in the index. For example, we can compute numbers over decades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_movies_2.resample('10Y').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* Connect to the Facebook database, and use the `MemberSince` variable from the `Profiles` table to plot the growth of Facebook users. Use the following information:\n",
    ">    user='student', \n",
    ">    password='dwdstudent2015', \n",
    ">    host = 'db.ipeirotis.org', \n",
    ">    port=3306, \n",
    ">    db='facebook'\n",
    "* (_Learn something new_) Use the [cumsum()](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.cumsum.html) function of Pandas and plot the total number of registered users over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "conn_string_fb = 'mysql://{user}:{password}@{host}:{port}/{db}?charset=utf8'.format(\n",
    "    user='student', \n",
    "    password='dwdstudent2015', \n",
    "    host = 'db.ipeirotis.org', \n",
    "    port=3306, \n",
    "    db='facebook',\n",
    "    encoding = 'utf-8'\n",
    ")\n",
    "engine_fb = create_engine(conn_string_fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Naive approach, fetch all the data first\n",
    "query = 'SELECT * FROM Profiles'\n",
    "df = pd.read_sql(query, con=engine_fb)\n",
    "pivot = df.pivot_table(\n",
    "    index='MemberSince',\n",
    "    values='ProfileID',\n",
    "    aggfunc='count'    \n",
    ")\n",
    "# Calculate weekly signups\n",
    "weekly_signups = pivot.resample('1W').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Push calculations into SQL\n",
    "query = 'SELECT MemberSince, COUNT(ProfileID) as signups FROM Profiles GROUP BY MemberSince'\n",
    "df = pd.read_sql(query, con=engine_fb)\n",
    "df.set_index(\"MemberSince\", inplace=True)\n",
    "weekly_signups = df.resample('1W').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "weekly_signups.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "df.cumsum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Further Examples with SQL and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's run a query to get the political views of Facebook users, broken down by gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conn_string_fb = 'mysql://{user}:{password}@{host}:{port}/{db}'.format(\n",
    "    user='student',\n",
    "    password='dwdstudent2015',\n",
    "    host='db.ipeirotis.org',\n",
    "    port=3306,\n",
    "    db='facebook')\n",
    "engine_fb = create_engine(conn_string_fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "polviews_by_gender = '''\n",
    "SELECT Sex, PoliticalViews, COUNT(*) AS cnt \n",
    "FROM Profiles \n",
    "WHERE Sex IS NOT NULL AND PoliticalViews IS NOT NULL \n",
    "GROUP BY Sex, PoliticalViews\n",
    "ORDER BY  PoliticalViews, Sex\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And let's get the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_sql(polviews_by_gender, con=engine_fb)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will now convert the PoliticalViews column into an **ordered Categorical variable**. This is not strictly necessary, but it will be useful later.\n",
    " It ensures that Political Views appear in an order according to their political spectrum, as opposed to alphabetical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.PoliticalViews = pd.Categorical(df.PoliticalViews,\n",
    "    categories = ['Very Liberal', 'Liberal', 'Moderate', 'Conservative', 'Very Conservative', 'Libertarian', 'Apathetic', 'Other'], \n",
    "    ordered=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's plot this!\n",
    "# Bleh, this is really fugly...\n",
    "# Remember that the index of the dataframe becomes the default x-axis\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pivot, baby!\n",
    "# Now the index contains the Political Views, which will be our x-axis\n",
    "dfp = pd.pivot_table(data = df, index='PoliticalViews', columns='Sex', values='cnt', aggfunc='sum')\n",
    "dfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfp.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Normalizing values in Pandas\n",
    "\n",
    "Now, let's see a bit how we can normalize the values in Pandas, by performing operations on the columns and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfp.sum() # sums across the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfp.T.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfp.sum(axis='index') # sums across the rows (equivalent to dfp.sum() and dfp.sum(axis=0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfp.sum(axis='columns') # this one sums across the columns (axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's normalize the columns, as we have more females than males, and it seems that there are always more women\n",
    "dfp_norm = dfp / dfp.sum()\n",
    "dfp_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfp_norm.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Instead of dfp / dfp.sum(), we can also use the .div() method, for dividing the entries with the sum()\n",
    "# Note that, by definition, the dfp / dfp.sum() operation divides  column-wise, not row-wise.\n",
    "dfp_norm = dfp.div( dfp.sum(), axis='columns' )\n",
    "dfp_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# And now let's do the division by normalizing the values row-wise,\n",
    "# to find the fraction of males/females within each political category\n",
    "dfp_norm2 = dfp.div( dfp.sum(axis='columns'), axis='index' ).sort_index()\n",
    "dfp_norm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfp_norm2.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfp_norm2.plot(kind='bar', rot=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "a. Use the tables `RelationshipStatus` and `LookingFor`, and show create a plot with a breakdown of what people in different relationship statuses are looking for. To make things more readable (and to practice a bit SQL), remove from the output all combinations that have less than 10 students in them. The plot can use the absolute counts.\n",
    "\n",
    "b. Normalize the results and plot again. To get experience with normalization, try to normalize both by Status (eg \"80% of the people who are in a relationship are looking for Friendship\") and by Relationship Status (eg \"70% of the people who are looking for Random Play are Single\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT R.Status, L.LookingFor, COUNT(*) AS cnt\n",
    "FROM Relationship R INNER JOIN LookingFor L ON R.ProfileID = L.ProfileID\n",
    "GROUP BY R.Status, L.LookingFor\n",
    "HAVING cnt>10\n",
    "'''\n",
    "df = pd.read_sql(query, con=engine_fb)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pivot = df.pivot_table(\n",
    "    index='Status',\n",
    "    columns='LookingFor',\n",
    "    values='cnt'\n",
    ")\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Raw numbers, by relationship status\n",
    "pivot.plot(kind='barh', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Raw numbers, by looking for (taking the transpose)\n",
    "pivot.T.plot(kind='barh', figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Every column (LookingFor) sums up to 1\n",
    "pivot / pivot.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Every column (LookingFor) sums up to 1\n",
    "(pivot / pivot.sum()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(pivot / pivot.sum()).T.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Every column (Relationship Status) sums up to 1\n",
    "pivot.T / pivot.T.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Facebook, Favorite Books, and Political views\n",
    "\n",
    "Now let's do an analysis that examines book preferences and how they correlated with political leanings.\n",
    "\n",
    "We will start by fetching the favorite books for students that declared themselves as Liberal or Conservative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "books = '''\n",
    "SELECT B.Book, P.PoliticalViews, COUNT(*) AS cnt \n",
    "FROM Profiles P JOIN FavoriteBooks B ON B.ProfileID = P.ProfileId  \n",
    "WHERE PoliticalViews IS NOT NULL AND B.Book IS NOT NULL \n",
    "      AND (PoliticalViews = 'Liberal' OR PoliticalViews = 'Conservative')\n",
    "AND B.Book IN (\n",
    "    SELECT Book \n",
    "    FROM FavoriteBooks B JOIN Profiles P ON B.ProfileID = P.ProfileId  \n",
    "    WHERE (P.PoliticalViews = 'Liberal' OR P.PoliticalViews = 'Conservative')\n",
    "    GROUP BY Book HAVING COUNT(DISTINCT P.ProfileID)>10\n",
    ")\n",
    "GROUP BY B.Book, P.PoliticalViews;\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_books = pd.read_sql(books, con=engine_fb)\n",
    "df_books.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(df_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfp = df_books.pivot_table(\n",
    "    index='Book', \n",
    "    columns='PoliticalViews', \n",
    "    values='cnt')\n",
    "dfp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Notice the `NaN` values for the entries where we had no users falling into that group. Since we will want to do calculations for these books as well, we will use the `fillna` command to fill these entries with a default value (in our case, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fill the NaN entries with the value 0 \n",
    "dfp = df_books.pivot_table(\n",
    "    index='Book', \n",
    "    columns='PoliticalViews', \n",
    "    values='cnt').fillna(0)\n",
    "dfp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Normalization**: We now want to normalize the entries before proceeding further. Let's take a look at the breakdown of political views in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "polviews = '''\n",
    "SELECT PoliticalViews, COUNT(*) AS cnt \n",
    "FROM facebook.Profiles\n",
    "GROUP BY PoliticalViews\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_polviews = pd.read_sql(polviews, con=engine_fb)\n",
    "df_polviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since we have many more conservatives than liberals, let's create a new column that calculates the **percentage** of liberal and conservative students that liked each book. For simplicity, we just enter directly the values 6461 (number of liberals) and 936 (number of conservatives). We add the `+1` in the numerator to avoid division by zero later on. _As practice, try to fetch the values 936 and 6461 directly from the database, and automate the calculation._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfp[\"Liberal_perc\"] = (dfp[\"Liberal\"] +1)  / 6461\n",
    "dfp[\"Conservative_perc\"] = (dfp[\"Conservative\"] +1)  / 936"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Lift\n",
    "\n",
    "Now that we have the normalized values, we can compute the **lift** for each book. The lift is the ratio between the percentage of liberals and the percentage of convervatives. A book with `lift==1` will be equally read by both conservatives and liberals. Books that have lifts significantly higher or lower than 1, reveal preferences to be read by one side of the political spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfp[\"lift_liberal\"] = dfp[\"Liberal_perc\"] / dfp[\"Conservative_perc\"]\n",
    "dfp[\"lift_conservative\"] = dfp[\"Conservative_perc\"]  / dfp[\"Liberal_perc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Log-odds\n",
    "\n",
    "One common tranformation is to take the `log` of the lift. We call this metric **log odds**. In that case, the `lift==1` corresponds to a `log_odds` of 0. Negative values indicate negative association, and positive values indicate positive association. A nice property of log-odds is that they are **additive**, which means that summing up log-odds makes (mathematical) sense, under some reasonably general conditions. (The details are beyond the scope of this course, but you can learn more in the data mining class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dfp[\"log_odds_liberal\"]      =  np.log(dfp[\"lift_liberal\"])\n",
    "dfp[\"log_odds_conservative\"] =  np.log(dfp[\"lift_conservative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_columns = [\"lift_liberal\", \"log_odds_liberal\", \"lift_conservative\", \"log_odds_conservative\", \"Liberal\", \"Conservative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "liberal_books = (dfp[show_columns]\n",
    "                 .sort_values(\"lift_liberal\", ascending=False)\n",
    "                 .head(10)\n",
    "                )\n",
    "liberal_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conservative_books = (dfp[show_columns]\n",
    "                      .sort_values(\"lift_conservative\", ascending=False)\n",
    "                      .head(10)\n",
    "                     )\n",
    "conservative_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot = conservative_books.lift_conservative.plot(kind='barh', figsize=(15,5))\n",
    "plot.set_xlabel(\"Lift for Conservatives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot = liberal_books.lift_liberal.plot(kind='barh', figsize=(15,5))\n",
    "plot.set_xlabel(\"Lift for Liberals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "We have seen how to compute the log-odds between liberal-conservative for each book. Given this information, we can try to estimate political leanings of students. You can do this by summing the log-odds of their favorite books. \n",
    "\n",
    "Steps: \n",
    "1. Create a table with the log-odds of the books. \n",
    "2. Join the table with the log-odds with the book preferences table.\n",
    "3. Sum the log-odds score for each student.\n",
    "\n",
    "Evaluation:\n",
    "* You have students that have declared their political preferences as Liberal, Conservative, Very Liberal, Very Conservative. Examine the scores for these students, to check how well this technique works. The simplest way is to compute the average (mean) log-odds for students that fall into the different groups. Alternatively, you can try to plot the full distribution of scores.\n",
    "* Calculate a score for each student that did not declare a political view but has listed Favorite Books.\n",
    "\n",
    "Notes: \n",
    "* You can do the work in MySQL or in Pandas. If you decide to work purely in Pandas, the [`merge`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html#pandas.DataFrame.merge) command allows you to perform joins between dataframes, in way similar to SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
