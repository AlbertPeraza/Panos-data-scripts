{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling and Extracting Data from Websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module uses a set of non-standard libraries, which need to be installed on your machine. By default, your instance should have these installed, but if this is not the case, type these in the Unix shell prompt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo -H apt-get -y install libxml2-dev libxslt-dev python3-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo -H pip3 install -U lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo -H pip3 install -U pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching the headlines from ESPN.com\n",
    "\n",
    "Let's start by trying to fetch the headlines from the site ESPN.com.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # This command allows us to fetch URLs\n",
    "from lxml import html # This module will allow us to parse the returned HTML/XML\n",
    "import pandas # To create a dataframe\n",
    "\n",
    "# Let's start by fetching the page, and parsing it\n",
    "url = \"http://www.espn.com/\"\n",
    "response = requests.get(url) # get the html of that url\n",
    "doc = html.fromstring(response.text) # parse it and create a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by getting the content of the `<title>` node from the site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return back a list of nodes that match\n",
    "# are called <title>......</title>\n",
    "results = doc.xpath('//title/text()')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `doc` variable is an `HtmlElement` object, and we can now use **XPath** queries to locate the elements that we need. (Depending on time, we may do in class a tutorial on XPath. For now, you can look at the [W3Schools tutorial](http://www.w3schools.com/xpath/xpath_nodes.asp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to find all the `<a ...> ... </a>` tags in the returned html, which store the links in the page, we issue the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = doc.xpath(\"//a\")\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = doc.xpath(\"//a\")\n",
    "# Iterates over all the links (this means all the nodes\n",
    "# that matched the //a XPath query) and prints the content\n",
    "# of the attribute href and the text for that node\n",
    "for link in links:\n",
    "    print(\"==================================\")\n",
    "    print(link.get(\"href\"), \"==>\", link.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnk = links[70]\n",
    "type(lnk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lnk` variable is again an HtmlElement. To get parts of the html element that we need, we can use the `get` method (e.g., to get the `href` attribute) and the `text` method (to get the text within the `<a>...</a>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnk.get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnk.text_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnk.text_content().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's revisit the _list comprehension_ approach that we discussed in the Python Primer session, for quickly constructing lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [lnk.get(\"href\") for lnk in doc.xpath('//a')]\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Use a list compresension approach, to get the text_content of all the URLs in the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now create a list where we put together text content and the URL for each link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's get the headlines..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine how we can get the data from the website. The key is to understand the structure of the HTML, where the data that we need is stored, and how to fetch the elements. Then, using the appropriate XPath queries, we will get what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # This command allows us to fetch URLs\n",
    "from lxml import html # This module will allow us to parse the returned HTML/XML\n",
    "import pandas # To create a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by fetching the page, and parsing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.espn.com/\"\n",
    "response = requests.get(url) # get the html of that url\n",
    "doc = html.fromstring(response.text) # parse it and create a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the `\"Right-Click > Inspect\"` option of Chrome,\n",
    "we right click on the headlines and select `\"Inspect\"`.\n",
    "This opens the source code.\n",
    "There we see that all under a `<div class=\"headlineStack\">` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlineNode = doc.xpath('//div[@class=\"headlineStack\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of that operation is a list with 6 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(headlineNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(headlineNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each headline is under a  `<li><a href=\"....\"></a>` tag.\n",
    "So, we get all the `<li><a ...>` tags within the `<div class=\"headlineStack\">`\n",
    "(which is stored in the \"`headlineNode`\" variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = headlineNode[1].xpath('.//li/a')\n",
    "len(headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the nodes with the conent in the headlines variable.\n",
    "We extract the text and the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\"Title\": a.text_content(), \"URL\": a.get(\"href\")} for a in headlines]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's create our dataframe, so that we can have a better view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pandas.DataFrame(data)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Of course, there are always more than one way to skin a cat...\n",
    "\n",
    "Alternatively, if we did not want to restrict ourselves to just the first headline box, we could write an alternative query, to get back all the headlines, that appear in an XPath `//div[@class=\"headlineStack\"]//li/a`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = doc.xpath('//div[@class=\"headlineStack\"]//li/a')\n",
    "data = [{\"Title\": a.text_content(), \"URL\": a.get(\"href\")} for a in headlines]\n",
    "df = pandas.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = doc.xpath('//a[@data-mptype=\"headline\"]')\n",
    "data = [{\"Title\": a.text_content(), \"URL\": a.get(\"href\")} for a in headlines]\n",
    "df = pandas.DataFrame(data)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Class Example: Crawl BuzzFeed\n",
    "\n",
    "* We will try to get the top articles that appear on Buzzfeed\n",
    "* We will grab the link for the article, the text of the title, the description, and the editor.\n",
    "* The results will be stored in a dataframe (we will see in detail what a dataframe is, in a couple of modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "resp = requests.get(\"http://www.buzzfeed.com\")\n",
    "doc = html.fromstring(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Solution for Buzzfeed (as of October 9, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>description</th>\n",
       "      <th>editor</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.buzzfeed.com/emmamcanaw/things-you...</td>\n",
       "      <td>These will *literally* take your hand and show...</td>\n",
       "      <td>Emma McAnaw</td>\n",
       "      <td>27 Things You Need If You've Basically Never D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.buzzfeed.com/newsletters?ref=hplego</td>\n",
       "      <td>We have newsletters to help you stay in the lo...</td>\n",
       "      <td>BuzzFeed</td>\n",
       "      <td>WTF Is Happening Today? BuzzFeed Newsletters C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.buzzfeed.com/ryanschocket2/ariana-...</td>\n",
       "      <td>I can't.</td>\n",
       "      <td>Ryan Schocket</td>\n",
       "      <td>This Is Why Ariana Grande Posted An Instagram ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.buzzfeednews.com/article/mattberma...</td>\n",
       "      <td>The UN ambassador and former South Carolina go...</td>\n",
       "      <td>Matt Berman</td>\n",
       "      <td>Nikki Haley Is Leaving Her Job As Ambassador T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.buzzfeed.com/hermionemellark/choos...</td>\n",
       "      <td>You're never fully dressed without perfume!</td>\n",
       "      <td>HermioneMellark</td>\n",
       "      <td>Choose Between These Aestetically Pleasing Per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.buzzfeed.com/chuphanginichandrakan...</td>\n",
       "      <td>\"Stop! I could've dropped my ________!\"</td>\n",
       "      <td>Cee Chandra</td>\n",
       "      <td>Can You Successfully Complete These Iconic Vines?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.buzzfeed.com/kylesheng/buzzfeeds-v...</td>\n",
       "      <td>#tryceratops #trymas</td>\n",
       "      <td>Kyle Sheng</td>\n",
       "      <td>The Try Guys Have MERCHANDISE And You Can Own ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.buzzfeednews.com/article/claudiaro...</td>\n",
       "      <td>The singer waded into politics on Sunday with ...</td>\n",
       "      <td>Claudia Rosenbaum</td>\n",
       "      <td>Taylor Swift's Instagram Post Has Caused A Mas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.buzzfeed.com/ellaleighton/plan-you...</td>\n",
       "      <td>Are you straight out of Genovia?</td>\n",
       "      <td>EllaLeighton</td>\n",
       "      <td>Plan Your Perfect Royal Life And We'll Tell Yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.buzzfeed.com/tessaz88/create-your-...</td>\n",
       "      <td>This quiz is out of this world!</td>\n",
       "      <td>tessaz88</td>\n",
       "      <td>Create Your Own Planet In 9 Steps And We'll Gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.buzzfeed.com/kitkittykat/plan-a-va...</td>\n",
       "      <td>Tis the season to get married!</td>\n",
       "      <td>kitkittykat</td>\n",
       "      <td>Plan A Vacation To Know When You're Getting Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://www.buzzfeed.com/flowerpower123/create...</td>\n",
       "      <td>An outfit to die for...</td>\n",
       "      <td>helpmejeffery</td>\n",
       "      <td>Create An Outfit At Boohoo And We'll Tell You ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.buzzfeed.com/chuphanginichandrakan...</td>\n",
       "      <td>Extra points if they ended up becoming your si...</td>\n",
       "      <td>Cee Chandra</td>\n",
       "      <td>Show Us Your Cute Diary Entry About The First ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://www.buzzfeed.com/mrcash/plan-the-perfe...</td>\n",
       "      <td>The perfect autumn day is just around the corner!</td>\n",
       "      <td>Happy</td>\n",
       "      <td>Plan The Perfect Autumn Day In 9 Steps And We'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.buzzfeednews.com/article/davidmack...</td>\n",
       "      <td>The CW released the photo almost two months af...</td>\n",
       "      <td>David Mack</td>\n",
       "      <td>Here’s The First Look At Ruby Rose As Batwoman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.buzzfeed.com/devrickiyota9/things-...</td>\n",
       "      <td>I know we're having the times of our lives, bu...</td>\n",
       "      <td>Devric Kiyota</td>\n",
       "      <td>28 Things To Help You Have The Apartment Every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.buzzfeed.com/andyneuenschwander/te...</td>\n",
       "      <td>\"Villain, I have done thy mother.\"</td>\n",
       "      <td>Andy Golder</td>\n",
       "      <td>Tell Us Your Pet Peeves And We'll Give You A S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.buzzfeed.com/elliewoodward/sophie-...</td>\n",
       "      <td>From baths with Maisie Williams to \"drone kill...</td>\n",
       "      <td>Ellie Woodward</td>\n",
       "      <td>Sophie Turner Revealed Some Behind-The-Scenes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://www.buzzfeednews.com/article/zahrahirj...</td>\n",
       "      <td>Experts predict potentially life-threatening s...</td>\n",
       "      <td>Zahra Hirji</td>\n",
       "      <td>Forecasters Are Expecting A Big Storm Surge Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>https://www.buzzfeed.com/kristatorres/heres-wh...</td>\n",
       "      <td>Your questions, answered.</td>\n",
       "      <td>Krista Torres</td>\n",
       "      <td>Here's Everything You Need To Know About Penis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  URL  \\\n",
       "0   https://www.buzzfeed.com/emmamcanaw/things-you...   \n",
       "1     https://www.buzzfeed.com/newsletters?ref=hplego   \n",
       "2   https://www.buzzfeed.com/ryanschocket2/ariana-...   \n",
       "3   https://www.buzzfeednews.com/article/mattberma...   \n",
       "4   https://www.buzzfeed.com/hermionemellark/choos...   \n",
       "5   https://www.buzzfeed.com/chuphanginichandrakan...   \n",
       "6   https://www.buzzfeed.com/kylesheng/buzzfeeds-v...   \n",
       "7   https://www.buzzfeednews.com/article/claudiaro...   \n",
       "8   https://www.buzzfeed.com/ellaleighton/plan-you...   \n",
       "9   https://www.buzzfeed.com/tessaz88/create-your-...   \n",
       "10  https://www.buzzfeed.com/kitkittykat/plan-a-va...   \n",
       "11  https://www.buzzfeed.com/flowerpower123/create...   \n",
       "12  https://www.buzzfeed.com/chuphanginichandrakan...   \n",
       "13  https://www.buzzfeed.com/mrcash/plan-the-perfe...   \n",
       "14  https://www.buzzfeednews.com/article/davidmack...   \n",
       "15  https://www.buzzfeed.com/devrickiyota9/things-...   \n",
       "16  https://www.buzzfeed.com/andyneuenschwander/te...   \n",
       "17  https://www.buzzfeed.com/elliewoodward/sophie-...   \n",
       "18  https://www.buzzfeednews.com/article/zahrahirj...   \n",
       "19  https://www.buzzfeed.com/kristatorres/heres-wh...   \n",
       "\n",
       "                                          description             editor  \\\n",
       "0   These will *literally* take your hand and show...        Emma McAnaw   \n",
       "1   We have newsletters to help you stay in the lo...           BuzzFeed   \n",
       "2                                            I can't.      Ryan Schocket   \n",
       "3   The UN ambassador and former South Carolina go...        Matt Berman   \n",
       "4         You're never fully dressed without perfume!    HermioneMellark   \n",
       "5             \"Stop! I could've dropped my ________!\"        Cee Chandra   \n",
       "6                                #tryceratops #trymas         Kyle Sheng   \n",
       "7   The singer waded into politics on Sunday with ...  Claudia Rosenbaum   \n",
       "8                    Are you straight out of Genovia?       EllaLeighton   \n",
       "9                     This quiz is out of this world!           tessaz88   \n",
       "10                     Tis the season to get married!        kitkittykat   \n",
       "11                            An outfit to die for...      helpmejeffery   \n",
       "12  Extra points if they ended up becoming your si...        Cee Chandra   \n",
       "13  The perfect autumn day is just around the corner!              Happy   \n",
       "14  The CW released the photo almost two months af...         David Mack   \n",
       "15  I know we're having the times of our lives, bu...      Devric Kiyota   \n",
       "16                 \"Villain, I have done thy mother.\"        Andy Golder   \n",
       "17  From baths with Maisie Williams to \"drone kill...     Ellie Woodward   \n",
       "18  Experts predict potentially life-threatening s...        Zahra Hirji   \n",
       "19                          Your questions, answered.      Krista Torres   \n",
       "\n",
       "                                             headline  \n",
       "0   27 Things You Need If You've Basically Never D...  \n",
       "1   WTF Is Happening Today? BuzzFeed Newsletters C...  \n",
       "2   This Is Why Ariana Grande Posted An Instagram ...  \n",
       "3   Nikki Haley Is Leaving Her Job As Ambassador T...  \n",
       "4   Choose Between These Aestetically Pleasing Per...  \n",
       "5   Can You Successfully Complete These Iconic Vines?  \n",
       "6   The Try Guys Have MERCHANDISE And You Can Own ...  \n",
       "7   Taylor Swift's Instagram Post Has Caused A Mas...  \n",
       "8   Plan Your Perfect Royal Life And We'll Tell Yo...  \n",
       "9   Create Your Own Planet In 9 Steps And We'll Gi...  \n",
       "10  Plan A Vacation To Know When You're Getting Ma...  \n",
       "11  Create An Outfit At Boohoo And We'll Tell You ...  \n",
       "12  Show Us Your Cute Diary Entry About The First ...  \n",
       "13  Plan The Perfect Autumn Day In 9 Steps And We'...  \n",
       "14     Here’s The First Look At Ruby Rose As Batwoman  \n",
       "15  28 Things To Help You Have The Apartment Every...  \n",
       "16  Tell Us Your Pet Peeves And We'll Give You A S...  \n",
       "17  Sophie Turner Revealed Some Behind-The-Scenes ...  \n",
       "18  Forecasters Are Expecting A Big Storm Surge Fr...  \n",
       "19  Here's Everything You Need To Know About Penis...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests # This command allows us to fetch URLs\n",
    "from lxml import html # This module will allow us to parse the returned HTML/XML\n",
    "import pandas\n",
    "\n",
    "# Let's start by fetching the page, and parsing it\n",
    "url = \"http://www.buzzfeed.com/\"\n",
    "response = requests.get(url) # get the html of that url\n",
    "doc = html.fromstring(response.text) # parse it and create a document\n",
    "\n",
    "articleNodes = doc.xpath(\"//div[@data-buzzblock='story-card']\") \n",
    "\n",
    "def parseArticleNode(article):\n",
    "    headline = article.xpath(\".//h2\")[0].text_content()\n",
    "    headline_link = article.xpath(\".//a\")[0].get(\"href\")\n",
    "    description = article.xpath(\".//p\")[0].text_content()\n",
    "    editor = article.xpath(\".//a[contains(@class,'card__meta__link')]/span\")[0].text_content().strip()\n",
    "\n",
    "    result = {\n",
    "        \"headline\": headline,\n",
    "        \"URL\" : headline_link,\n",
    "        \"description\" : description,\n",
    "        \"editor\" : editor\n",
    "    }\n",
    "    return result\n",
    "\n",
    "data = [parseArticleNode(article) for article in articleNodes]\n",
    "df = pandas.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
